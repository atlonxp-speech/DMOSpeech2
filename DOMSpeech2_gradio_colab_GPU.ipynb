{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "1 of 6: install correct Python version; setup virtual environment; install pip\n"
      ],
      "metadata": {
        "id": "BmSv_cpUFaeH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!apt-get update -y\n",
        "!apt-get install python3.10 python3.10-distutils python3.10-venv -y\n"
      ],
      "metadata": {
        "id": "0ANV_E-dnX_y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python3.10 -m venv /content/dmo_env\n"
      ],
      "metadata": {
        "id": "OPHkJFein2wD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "source /content/dmo_env/bin/activate\n",
        "\n",
        "# Explicitly install pip within your existing venv\n",
        "python -m ensurepip --upgrade\n",
        "\n",
        "# Explicitly verify pip installation\n",
        "which pip\n",
        "pip --version\n"
      ],
      "metadata": {
        "id": "96OeE6-YnX6G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "2 of 6: ensure GPU is available (e.g., runtime= \"Connect T4\") before cloning/downloading; show initial state of GPU"
      ],
      "metadata": {
        "id": "AyHITIWMFn6F"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "klnBtajqEfMG"
      },
      "outputs": [],
      "source": [
        "# GPU check explicitly\n",
        "import torch\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    device_name = torch.cuda.get_device_name(0)\n",
        "    print(f\"‚úÖ GPU is available: {device_name}\")\n",
        "else:\n",
        "    print(\"‚ùå GPU is not available. Ensure GPU runtime is enabled.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "metadata": {
        "id": "YLA_9M-yEwew"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "3 of 6: clone (fork of) DMOSpeech2"
      ],
      "metadata": {
        "id": "QFR7jrHdF43y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# Explicitly move to a known good directory\n",
        "os.chdir('/content')\n",
        "\n",
        "# Verify explicitly you're back in a good state\n",
        "print(\"Current working directory explicitly set to:\", os.getcwd())\n",
        "\n",
        "!git clone https://github.com/wrightmikea/DMOSpeech2.git\n",
        "%cd DMOSpeech2\n",
        "!echo \"pwd=`pwd`\""
      ],
      "metadata": {
        "id": "vLkhec1bF0NJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "4 of 6: install requirements into activated virtual environment"
      ],
      "metadata": {
        "id": "td4jzBA6Kzs1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "!source /content/dmo_env/bin/activate && pip install -r requirements.txt\n"
      ],
      "metadata": {
        "id": "txQY2CUZkVkJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "5 of 6: fetch the DMOSpeech2 PyTorch model checkpoints from Huggingface into the cloned (forked) repo"
      ],
      "metadata": {
        "id": "18FRx780OFOJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.chdir('/content/DMOSpeech2')\n",
        "\n",
        "# Explicitly download checkpoints\n",
        "!mkdir -p ckpts\n",
        "!wget -O ckpts/model_85000.pt https://huggingface.co/yl4579/DMOSpeech2/resolve/main/model_85000.pt\n",
        "!wget -O ckpts/model_1500.pt https://huggingface.co/yl4579/DMOSpeech2/resolve/main/model_1500.pt\n"
      ],
      "metadata": {
        "id": "9qzsbPAzOD8K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "6 of 6: verify environment; run the app to show the Gradio UI"
      ],
      "metadata": {
        "id": "Qbu6tgmoPSkU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import subprocess\n",
        "\n",
        "venv_path = \"/content/dmo_env\"\n",
        "python_bin = f\"{venv_path}/bin/python\"\n",
        "\n",
        "def run_command(cmd):\n",
        "    result = subprocess.run(cmd, shell=True, stdout=subprocess.PIPE,\n",
        "                            stderr=subprocess.PIPE, text=True)\n",
        "    return result.stdout.strip() if result.stdout else result.stderr.strip()\n",
        "\n",
        "print(\"üîç Explicit verification after corrected installation:\")\n",
        "\n",
        "# Verify PyTorch explicitly\n",
        "torch_version = run_command(f'{python_bin} -c \"import torch; print(torch.__version__)\"')\n",
        "print(f\"‚Ä¢ PyTorch version explicitly: {torch_version}\")\n",
        "\n",
        "# Verify torchaudio explicitly\n",
        "torchaudio_version = run_command(f'{python_bin} -c \"import torchaudio; print(torchaudio.__version__)\"')\n",
        "print(f\"‚Ä¢ torchaudio version explicitly: {torchaudio_version}\")\n",
        "\n",
        "# Verify CUDA explicitly\n",
        "cuda_available = run_command(f'{python_bin} -c \"import torch; print(torch.cuda.is_available())\"')\n",
        "print(f\"‚Ä¢ CUDA explicitly available: {cuda_available}\")\n",
        "\n",
        "# Explicitly confirm accelerate package explicitly installed\n",
        "accelerate_version = run_command(f'{python_bin} -c \"import accelerate; print(accelerate.__version__)\"')\n",
        "print(f\"‚Ä¢ accelerate explicitly installed: {accelerate_version}\")\n"
      ],
      "metadata": {
        "id": "c_bnhov0r4no"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!source /content/dmo_env/bin/activate && python --version && python scripts/colab-gradio.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YuzDYHzHBmik",
        "outputId": "987f72ee-9829-4206-e4cb-c59a703381dc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Python 3.10.12\n",
            "Building prefix dict from the default dictionary ...\n",
            "Loading model from cache /tmp/jieba.cache\n",
            "Loading model cost 1.114 seconds.\n",
            "Prefix dict has been built successfully.\n",
            "Word segmentation module jieba initialized.\n",
            "\n",
            "Download Vocos from huggingface charactr/vocos-mel-24khz\n",
            "* Running on local URL:  http://127.0.0.1:7860\n",
            "* Running on public URL: https://3b3209c351ede17406.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n",
            "Converting audio...\n",
            "Using custom reference text...\n",
            "\n",
            "ref_text   this is a test. \n",
            "/content/dmo_env/lib/python3.10/site-packages/gradio/processing_utils.py:777: UserWarning: Trying to convert audio automatically from float32 to 16-bit int format.\n",
            "  warnings.warn(warning.format(data.dtype))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!curl http://localhost:7860/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w2Hha457AFF-",
        "outputId": "b687b55d-6886-4ac1-8bf8-ab7f5e087750"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "curl: (7) Failed to connect to localhost port 7860 after 0 ms: Connection refused\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pwd\n",
        "!ls\n",
        "!grep -i launch scripts/*.*\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mLsjDYLaEY1Z",
        "outputId": "a6405812-fe2c-4b39-8705-39d76022567e"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/DMOSpeech2\n",
            "ckpts\t   data  LICENSE\t     README.md\t       scripts\t\t   src\n",
            "CLAUDE.md  info  original-README.md  requirements.txt  setup-source-me.sh\n",
            "scripts/local-gradio.py:    demo.launch(server_name=\"127.0.0.1\", server_port=7860)\n",
            "scripts/remote-gradio.py:    demo.launch(server_name=\"0.0.0.0\", server_port=7860)\n"
          ]
        }
      ]
    }
  ]
}